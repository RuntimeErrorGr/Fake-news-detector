{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF8o7Bt_PZRi",
        "outputId": "c6f3a6d6-b21b-441e-bb1c-918bba26a8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.109.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.35.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.16.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install scikit-learn\n",
        "!pip install datasets torchmetrics gradio kaggle typing-extensions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "source_folder_path = '/content/drive/My Drive/fac/master/bigData'\n",
        "destination_folder_path = '/content/bigData'\n",
        "\n",
        "shutil.copytree(source_folder_path, destination_folder_path)\n",
        "directory_path = '/content/bigData/big_data_project'\n",
        "\n",
        "os.chdir(directory_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6HH7kuV95xQ",
        "outputId": "526f7548-9476-4277-9e4d-7161ed58faa2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/fac/master/bigData/big_data_project')\n",
        "\n",
        "from data_pipeline import get_dataloaders, text_preprocess, isot_clean\n",
        "\n",
        "checkpoint_path = '../../drive/My Drive/fac/master/bigData/trained_model/'\n",
        "trainset_path = 'drive/My Drive/fac/master/bigData/big_data_project/train.csv'\n",
        "testset_path = '../../drive/My Drive/fac/master/bigData/big_data_project/test.csv'\n",
        "\n",
        "(train_loader, val_loader, test_loader), (tokenizer, vocab) = get_dataloaders(dataset_id=\"fake_news\", model=\"bert\")\n",
        "\n",
        "def inference(model, input_str, max_len=512, isot=False, device=\"cpu\", processor=None):\n",
        "    if isot:\n",
        "        input_str = isot_clean(input_str)\n",
        "    # tokenizer, vocab = processor\n",
        "    preprocessed = text_preprocess(input_str)\n",
        "    token_ids = vocab(tokenizer(preprocessed))\n",
        "    if not token_ids:\n",
        "        return np.array([0.5, 0.5])\n",
        "    input_ids = torch.tensor(token_ids[:max_len]).unsqueeze(0).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(input_ids).squeeze().detach().cpu().item()\n",
        "\n",
        "    return np.array([1 - prediction, prediction])\n",
        "\n",
        "###################################################################################################\n",
        "\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "\n",
        "model = BERTModel(num_labels=2)  # Assuming binary classification, adjust num_labels accordingly\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "criterion.to(device)\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_labels, train_preds = [], []\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = batch[0]\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        input_ids = inputs['input_ids'].to(device)\n",
        "        attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "        train_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Average training loss\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_labels, val_preds = [], []\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch[0]\n",
        "            labels = batch[1].to(device)\n",
        "\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "            attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "            val_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "    # Average validation loss\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        checkpoint_full_path = checkpoint_path + \"bert_fake_news.tar\"\n",
        "        model_config = {\n",
        "            \"num_labels\" : 2\n",
        "        }\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch+1,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"model_config\": model_config,\n",
        "                \"optimizer_state_dict\": optimizer.state_dict()\n",
        "            },\n",
        "            checkpoint_full_path\n",
        "        )\n",
        "\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, '\n",
        "          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, '\n",
        "          f'Patience Counter: {patience_counter}')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if patience_counter >= patience:\n",
        "        print(f'Early stopping at epoch {epoch + 1} due to no improvement in validation loss.')\n",
        "        break\n",
        "\n",
        "\n",
        "# # Testing\n",
        "# model.eval()\n",
        "# test_labels, test_preds = [], []\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         inputs = batch[0]\n",
        "#         labels = batch[1].to(device)\n",
        "\n",
        "#         input_ids = inputs['input_ids'].to(device)\n",
        "#         attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "#         logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "#         test_labels.extend(labels.cpu().numpy())\n",
        "#         test_preds.extend(predictions.cpu().numpy())\n",
        "\n",
        "# test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "# print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "model.eval()\n",
        "# results = []\n",
        "\n",
        "submission_ids = []\n",
        "submission_labels = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        text, _, _, ids = batch\n",
        "        text = text.to(device)\n",
        "        submission_ids.extend(ids.tolist())\n",
        "        predictions = model(text).round().int().squeeze().detach().cpu().tolist()\n",
        "        submission_labels.extend(predictions)\n",
        "\n",
        "df_test = pd.read_csv(testset_path)\n",
        "for idx in df_test[\"id\"]:\n",
        "     if idx not in submission_ids:\n",
        "        submission_ids.append(idx)\n",
        "        submission_labels.append(1)\n",
        "\n",
        "submission_df = pd.DataFrame.from_dict({\"id\": submission_ids, \"label\": submission_labels})\n",
        "submission_df.to_csv(\"submission.csv\",index=False)\n",
        "\n",
        "# for batch in test_loader:\n",
        "#     inputs = batch[0]\n",
        "#     print(batch)\n",
        "#\n",
        "#     input_ids = inputs['input_ids'].to(device)\n",
        "#     attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "#     logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#     predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "#     for id, text, label in zip(batch[2], batch[0], predictions.cpu().numpy()):\n",
        "#         results.append({'id': id.item(), 'text': text, 'predicted_label': label.item()})\n",
        "\n",
        "# result_df = pd.DataFrame(results)\n",
        "# result_df.to_csv('predictions.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "3lVJDTWzPc-o",
        "outputId": "6b1dd7df-3f5f-4c13-fa1e-79f949b26543"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Training Loss: 0.2113, Training Accuracy: 0.9122, Validation Loss: 0.0718, Validation Accuracy: 0.9738, Patience Counter: 0\n",
            "Epoch 2/5, Training Loss: 0.0606, Training Accuracy: 0.9797, Validation Loss: 0.0798, Validation Accuracy: 0.9767, Patience Counter: 1\n",
            "Epoch 3/5, Training Loss: 0.0296, Training Accuracy: 0.9902, Validation Loss: 0.0529, Validation Accuracy: 0.9845, Patience Counter: 0\n",
            "Epoch 4/5, Training Loss: 0.0197, Training Accuracy: 0.9934, Validation Loss: 0.0562, Validation Accuracy: 0.9874, Patience Counter: 1\n",
            "Epoch 5/5, Training Loss: 0.0087, Training Accuracy: 0.9973, Validation Loss: 0.0657, Validation Accuracy: 0.9835, Patience Counter: 2\n",
            "Early stopping at epoch 5 due to no improvement in validation loss.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-75ef0911e8f6>\u001b[0m in \u001b[0;36m<cell line: 204>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0msubmission_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "results = []\n",
        "pred = []\n",
        "for batch in test_loader:\n",
        "    inputs = batch[0]\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "    for id, label in zip(batch[2], predictions.cpu().numpy()):\n",
        "        results.append({'id': id.item(), 'label': label.item()})\n",
        "        pred.append(label.item())\n",
        "    # result_df = pd.DataFrame({'id': batch[2], 'predicted_label': predictions.cpu().numpy()})\n",
        "print(len(pred))\n",
        "result_df = pd.DataFrame(results)\n",
        "result_df.to_csv('predictions.csv', index=False)\n",
        "# result_df.to_csv('result_predictions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRYrCHNj2s6d",
        "outputId": "9468ab18-7e7a-40e5-e065-ddf447bd2e81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plots\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/fac/master/bigData/big_data_project')\n",
        "\n",
        "from data_pipeline import get_dataloaders, text_preprocess, isot_clean\n",
        "\n",
        "checkpoint_path = '../../drive/My Drive/fac/master/bigData/trained_model/'\n",
        "\n",
        "(train_loader, val_loader, test_loader), (tokenizer, vocab) = get_dataloaders(dataset_id=\"fake_news\", model=\"bert\")\n",
        "\n",
        "train_dataset = train_loader.dataset\n",
        "data = [sample for sample in train_dataset]\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "inputs = df[0].tolist()\n",
        "labels = df[1].tolist()\n",
        "df = df.rename(columns={0: 'input_data', 1: 'label', 2: 'id'})\n",
        "\n",
        "# Plotting label distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['label'].value_counts().plot(kind='bar', color=['skyblue', 'orange'])\n",
        "plt.title('Label Distribution fake_news dataset')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "D6ae4NXYY2Lx",
        "outputId": "bcff7fe2-3ca2-484b-d874-187e9558fa08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                       0  1      2\n",
            "0      {'input_ids': [tensor(101), tensor(2215), tens...  0   6969\n",
            "1      {'input_ids': [tensor(101), tensor(1014), tens...  1   4013\n",
            "2      {'input_ids': [tensor(101), tensor(3487), tens...  0   7071\n",
            "3      {'input_ids': [tensor(101), tensor(2355), tens...  1   9478\n",
            "4      {'input_ids': [tensor(101), tensor(6877), tens...  0   1901\n",
            "...                                                  ... ..    ...\n",
            "19318  {'input_ids': [tensor(101), tensor(2175), tens...  1  11284\n",
            "19319  {'input_ids': [tensor(101), tensor(3422), tens...  1  11964\n",
            "19320  {'input_ids': [tensor(101), tensor(1999), tens...  0   5390\n",
            "19321  {'input_ids': [tensor(101), tensor(2845), tens...  1    860\n",
            "19322  {'input_ids': [tensor(101), tensor(2002), tens...  1  15795\n",
            "\n",
            "[19323 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/fac/master/bigData/big_data_project')\n",
        "\n",
        "from data_pipeline import get_dataloaders, text_preprocess, isot_clean\n",
        "\n",
        "checkpoint_path = '../../drive/My Drive/fac/master/bigData/trained_model/'\n",
        "\n",
        "(train_loader, val_loader, test_loader), (tokenizer, vocab) = get_dataloaders(dataset_id=\"fake_news\", model=\"bert\")\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "checkpoint = torch.load(checkpoint_path + 'bert_fake_news.tar')\n",
        "\n",
        "adjusted_state_dict = {key.replace('bert.', ''): value for key, value in checkpoint['model_state_dict'].items()}\n",
        "\n",
        "model.load_state_dict(adjusted_state_dict)\n",
        "model.eval()\n",
        "\n",
        "results = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    inputs = batch[0]\n",
        "\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "    for id, text, label in zip(batch[2], batch[0]['text'], predictions.cpu().numpy()):\n",
        "        results.append({'id': id.item(), 'text': text, 'predicted_label': label.item()})\n",
        "\n",
        "result_df = pd.DataFrame(results)\n",
        "result_df.to_csv('predictions.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "gY9bTNewi8GU",
        "outputId": "d4d79e7c-91f9-4d88-cd56-791af2a49a3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3e62ab0a3efd>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0madjusted_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjusted_state_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertForSequenceClassification:\n\tMissing key(s) in state_dict: \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.wei...\n\tUnexpected key(s) in state_dict: \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"e..."
          ]
        }
      ]
    }
  ]
}